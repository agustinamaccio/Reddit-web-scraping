{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07588ab",
   "metadata": {},
   "source": [
    "### Case Study 1\n",
    "\n",
    "Assume you are new to the data science field, and you want to find out what real practitioners and wannabe data scientists are concerned about. One place where you may find such information is Twitter. However, Twitter users often use their real identities and may have reservations about sharing all their opinions publicly. Another place where such information maybe found is the datascience subreddit on Reddit.com (https://www.reddit.com/r/datascience/). Users are assumed to be anonymous and they are more likely to share their opinions without reservations. To find out common concerns among the datascience subreddit users, it might be a good idea to collect the top 100 posts in the subreddit in the year 2021. You might also collect the top 3 comments of each of those posts. In this case study, we will do exactly that. Specific details can be found in the next few cells. \n",
    "\n",
    "This data can be used for many different projects. However, we are only going to focus on the \"data gathering\" part. We will also do some cleaning.\n",
    "\n",
    "**Note**: This case study contributes 12.5% to your overall grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4b624",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "###  15 points\n",
    "\n",
    "\n",
    "**Description:** \n",
    "\n",
    "Learn about the **praw** package for Python and learn how you can use it to load reddit posts, comments etc. on a Jupyter Notebook. Do a Google search. You might find tutorials. It is okay to use them. You may need to use secret keys for this part. For that you will need to open a Reddit account. You can use a throwaway account for this purpose. Write your code in the cell below. Any code you write to retrieve data from Reddit can go there.\n",
    "\n",
    "**Grading criteria:** \n",
    "\n",
    "The code for this step must be correct. Otherwise, the next steps cannot be completed. In that case, the next steps will not be graded. If you receive a praw object from the data science subreddit, you will get full 15 points.' Other methods may be considered, but not encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf6c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': '612393978331-zMNMVTwxwhW6X62WgcApKaWyg98HoQ',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 3600,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 1 goes here\n",
    "\n",
    "# requests module = HTTP library. It allows to send HTTP requests using Python\n",
    "import requests\n",
    "import requests.auth #Reddit requires HHTP Basic Auth\n",
    "\n",
    "client_auth = requests.auth.HTTPBasicAuth('ZRB4PJGg69F5VA4fyZoTRw', 'e7zeyU4iRvXnLpb-x3hr3cJfLxPGdQ') # parameters = ('user', 'pass')\n",
    "\n",
    "# I delete my user name and password for security purposes. If anyone want to run this code, that person should create a Reddit API\n",
    "post_data = {\"grant_type\":\"password\", \"username\":\"xxxx\", \"password\":\"xxxxx\"} \n",
    "\n",
    "# unique and descriptive user agent:\n",
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "\n",
    "# acquiring a token / post() method to send some data to the server\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "\n",
    "# returns a JSON object \n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32e031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'is_employee': False,\n",
       " 'seen_layout_switch': False,\n",
       " 'has_visited_new_profile': False,\n",
       " 'pref_no_profanity': True,\n",
       " 'has_external_account': False,\n",
       " 'pref_geopopular': '',\n",
       " 'seen_redesign_modal': False,\n",
       " 'pref_show_trending': True,\n",
       " 'subreddit': {'default_set': True,\n",
       "  'user_is_contributor': False,\n",
       "  'banner_img': '',\n",
       "  'restrict_posting': True,\n",
       "  'user_is_banned': False,\n",
       "  'free_form_reports': True,\n",
       "  'community_icon': None,\n",
       "  'show_media': True,\n",
       "  'icon_color': '#7EED56',\n",
       "  'user_is_muted': False,\n",
       "  'display_name': 'u_DS_501_A',\n",
       "  'header_img': None,\n",
       "  'title': '',\n",
       "  'coins': 0,\n",
       "  'previous_names': [],\n",
       "  'over_18': False,\n",
       "  'icon_size': [256, 256],\n",
       "  'primary_color': '',\n",
       "  'icon_img': 'https://www.redditstatic.com/avatars/defaults/v2/avatar_default_3.png',\n",
       "  'description': '',\n",
       "  'submit_link_label': '',\n",
       "  'header_size': None,\n",
       "  'restrict_commenting': False,\n",
       "  'subscribers': 0,\n",
       "  'submit_text_label': '',\n",
       "  'is_default_icon': True,\n",
       "  'link_flair_position': '',\n",
       "  'display_name_prefixed': 'u/DS_501_A',\n",
       "  'key_color': '',\n",
       "  'name': 't5_50dnu0',\n",
       "  'is_default_banner': True,\n",
       "  'url': '/user/DS_501_A/',\n",
       "  'quarantine': False,\n",
       "  'banner_size': None,\n",
       "  'user_is_moderator': True,\n",
       "  'accept_followers': True,\n",
       "  'public_description': '',\n",
       "  'link_flair_enabled': False,\n",
       "  'disable_contributor_requests': False,\n",
       "  'subreddit_type': 'user',\n",
       "  'user_is_subscriber': False},\n",
       " 'pref_show_presence': True,\n",
       " 'snoovatar_img': '',\n",
       " 'snoovatar_size': None,\n",
       " 'gold_expiration': None,\n",
       " 'has_gold_subscription': False,\n",
       " 'is_sponsor': False,\n",
       " 'num_friends': 0,\n",
       " 'features': {'mod_service_mute_writes': True,\n",
       "  'promoted_trend_blanks': True,\n",
       "  'show_amp_link': True,\n",
       "  'chat': True,\n",
       "  'is_email_permission_required': False,\n",
       "  'mod_awards': True,\n",
       "  'mweb_xpromo_revamp_v3': {'owner': 'growth',\n",
       "   'variant': 'treatment_2',\n",
       "   'experiment_id': 480},\n",
       "  'mweb_xpromo_revamp_v2': {'owner': 'growth',\n",
       "   'variant': 'control_2',\n",
       "   'experiment_id': 457},\n",
       "  'awards_on_streams': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_ios': True,\n",
       "  'chat_subreddit': True,\n",
       "  'cookie_consent_banner': True,\n",
       "  'modlog_copyright_removal': True,\n",
       "  'do_not_track': True,\n",
       "  'mod_service_mute_reads': True,\n",
       "  'chat_user_settings': True,\n",
       "  'use_pref_account_deployment': True,\n",
       "  'mweb_xpromo_interstitial_comments_ios': True,\n",
       "  'mweb_link_tab': {'owner': 'growth',\n",
       "   'variant': 'control_2',\n",
       "   'experiment_id': 404},\n",
       "  'premium_subscriptions_table': True,\n",
       "  'mweb_xpromo_interstitial_comments_android': True,\n",
       "  'noreferrer_to_noopener': True,\n",
       "  'chat_group_rollout': True,\n",
       "  'resized_styles_images': True,\n",
       "  'spez_modal': True,\n",
       "  'mweb_xpromo_modal_listing_click_daily_dismissible_android': True,\n",
       "  'expensive_coins_package': True},\n",
       " 'can_edit_name': False,\n",
       " 'verified': True,\n",
       " 'new_modmail_exists': None,\n",
       " 'pref_autoplay': True,\n",
       " 'coins': 0,\n",
       " 'has_paypal_subscription': False,\n",
       " 'has_subscribed_to_premium': False,\n",
       " 'id': '7tbvk9uj',\n",
       " 'has_stripe_subscription': False,\n",
       " 'oauth_client_id': 'ZRB4PJGg69F5VA4fyZoTRw',\n",
       " 'can_create_subreddit': True,\n",
       " 'over_18': False,\n",
       " 'is_gold': False,\n",
       " 'is_mod': False,\n",
       " 'awarder_karma': 0,\n",
       " 'suspension_expiration_utc': None,\n",
       " 'has_verified_email': True,\n",
       " 'is_suspended': False,\n",
       " 'pref_video_autoplay': True,\n",
       " 'in_chat': True,\n",
       " 'has_android_subscription': False,\n",
       " 'in_redesign_beta': True,\n",
       " 'icon_img': 'https://www.redditstatic.com/avatars/defaults/v2/avatar_default_3.png',\n",
       " 'has_mod_mail': False,\n",
       " 'pref_nightmode': False,\n",
       " 'awardee_karma': 0,\n",
       " 'hide_from_robots': False,\n",
       " 'password_set': True,\n",
       " 'link_karma': 1,\n",
       " 'force_password_reset': False,\n",
       " 'total_karma': 1,\n",
       " 'seen_give_award_tooltip': False,\n",
       " 'inbox_count': 0,\n",
       " 'seen_premium_adblock_modal': False,\n",
       " 'pref_top_karma_subreddits': True,\n",
       " 'has_mail': False,\n",
       " 'pref_show_snoovatar': False,\n",
       " 'name': 'DS_501_A',\n",
       " 'pref_clickgadget': 5,\n",
       " 'created': 1631041891.0,\n",
       " 'gold_creddits': 0,\n",
       " 'created_utc': 1631041891.0,\n",
       " 'has_ios_subscription': False,\n",
       " 'pref_show_twitter': False,\n",
       " 'in_beta': False,\n",
       " 'comment_karma': 0,\n",
       " 'accept_followers': True,\n",
       " 'has_subscribed': False,\n",
       " 'linked_identities': [],\n",
       " 'seen_subreddit_chat_ftux': False}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the token to access headers\n",
    "headers = {\"Authorization\": \"bearer 612393978331-zMNMVTwxwhW6X62WgcApKaWyg98HoQ\", \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers)\n",
    "print(response.status_code) \n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04f6a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#I have installed praw module using \"pip install praw\" in Anaconda Prompt\n",
    "import praw\n",
    "\n",
    "# create a read-only Reddit instance to retrieve public information from Reddit\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"ZRB4PJGg69F5VA4fyZoTRw\",\n",
    "    client_secret=\"e7zeyU4iRvXnLpb-x3hr3cJfLxPGdQ\",\n",
    "    user_agent= headers,\n",
    ")\n",
    "\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea70d5",
   "metadata": {},
   "source": [
    "## Step 2: \n",
    "### 10 + 20 + 10 + 15 + 5 + 5 = 65 points\n",
    "\n",
    "**Description**:\n",
    "Once you have the mechanism in place to retrieve data from Reddit, you next step is to determine which parts of the data is necessary. For this case study, collect only the top posts from the year 2021. Also consider if the score of each post was above 50 or not. If the score was below 50, it might not have been an important post. Do not consider those posts. \n",
    "\n",
    "You may also observe that sometimes posts with memes or jokes get a lot of 'upvotes,' and because of that they may  have high scores, but they may not be useful for this case study. To address this problem, you will simply get rid of any post that has fewer than 5 words in the title. \n",
    "\n",
    "You will also notice that praw returns time as an integer. It is inconvenient for us to read time like that. You may want to convert the integer time to human readable time. You do not need to mention hours, minutes, or seconds. Just year, month and date is enough.\n",
    "\n",
    "**Grading Criteria:**\n",
    "* posts are only from the year 2021: 10 points\n",
    "* the integer time format converted into year-month-day: 20 points\n",
    "* only posts with scores more than 50 were considered: 10 points\n",
    "* only post titles with more than 5 words were kept: 15 points\n",
    "* minimum 100 posts were collected: 5 points\n",
    "* three comments collected for each post: 5 points\n",
    "\n",
    "Note: All six grading criteria can be satified by writing one line or many lines of code. It does not matter. As long as your code satisfies the six criteria (in one line or many lines), you will get full points. Otherwise, you will get partial credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f44da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for step 2 goes here\n",
    "\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "\n",
    "# obtain a Subreddit instance about Data Science\n",
    "subreddit = reddit.subreddit('datascience')\n",
    "\n",
    "# top() method to sort posts based on top \n",
    "top_data_science = subreddit.top(limit= 500) \n",
    "\n",
    "\n",
    "# Function to convert integer time format into year-month-day\n",
    "def time_converter (time_in_int):\n",
    "    time = time_in_int\n",
    "    time_converted = datetime.utcfromtimestamp(time)\n",
    "    return time_converted\n",
    "\n",
    "# Function to check if title has emojis\n",
    "def has_emoji(text):\n",
    "    for letter in text:\n",
    "        if (letter in emoji.UNICODE_EMOJI[\"en\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "list_submission_date = []\n",
    "list_submission_score = []\n",
    "list_submission_title = []\n",
    "list_submission_com1 = []\n",
    "list_submission_com2 = []\n",
    "list_submission_com3 = []\n",
    "\n",
    "for submission in top_data_science:\n",
    "    time = time_converter(submission.created_utc) # call to time_converter() function\n",
    "    limit_time = datetime.fromisoformat('2021-01-01 00:00:00') # set the time limit to 01/01/2021\n",
    "    score = submission.score # assing the post's score to a variable\n",
    "    if time >= limit_time and score > 50: \n",
    "        title = submission.title\n",
    "        words = len(title.split())\n",
    "        emojis = has_emoji(title) # call to has_emoji() function\n",
    "        if words > 5 and emojis == False:\n",
    "            list_submission_date.append(time)\n",
    "            list_submission_score.append(submission.score)\n",
    "            list_submission_title.append(submission.title)\n",
    "            \n",
    "            submission.comments.replace_more(limit=0) # method replace_more() replaces or removes MoreComments objects (replies)\n",
    "            submission.comments_sort = 'top'\n",
    "            list_submission_com1.append(submission.comments[0].body)\n",
    "            list_submission_com2.append(submission.comments[1].body)\n",
    "            list_submission_com3.append(submission.comments[2].body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87812b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2021, 7, 12, 14, 39, 38), datetime.datetime(2021, 2, 14, 3, 0, 3), datetime.datetime(2021, 4, 8, 19, 22, 16), datetime.datetime(2021, 7, 26, 13, 6, 23), datetime.datetime(2021, 3, 25, 13, 19, 39), datetime.datetime(2021, 8, 18, 6, 34, 5), datetime.datetime(2021, 4, 12, 4, 9, 25), datetime.datetime(2021, 8, 19, 16, 1, 5), datetime.datetime(2021, 6, 20, 13, 58, 29), datetime.datetime(2021, 6, 7, 14, 33, 48)]\n",
      "[2646, 2184, 1729, 1630, 1396, 1371, 1217, 1202, 1131, 1019]\n",
      "['how about that data integrity yo', 'I created a four-page Data Science Cheatsheet to assist with exam reviews, interview prep, and anything in-between', \"I just got offered a data science internship with Amazon. I've been lurking on the sub for 3 years and just wanted to thank the folks who put together stats/ml cheat sheets.\", 'Me showing off a suspiciously well-performing model [OC]', 'Alan Turing is the new face on the British £50 note', 'Very proud of my CS book collection.', 'I found a research paper that is almost entirely my copied-and-pasted Kaggle work?', 'The Key Word in Data Science is Science, not Data', 'Hi! I just expanded the Data Science Cheatsheet to five pages, added material on Time Series, Statistics, and A/B Testing, and landed my first full-time job', \"Data Science and Data Analytics is becoming ultra glorified / romanticized, and I don't think people are really told what they are getting into.\"]\n"
     ]
    }
   ],
   "source": [
    "#checking that the data has been loaded correctly\n",
    "print(list_submission_date[0:10])\n",
    "print(list_submission_score[0:10])\n",
    "print(list_submission_title[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136d051",
   "metadata": {},
   "source": [
    "## Step 3: \n",
    "### 10 points\n",
    "\n",
    "Save the data on your local disk. You may have used lists or similar data structures for the intial porcessing. Convert that data structure into a Pandas dataframe. Save the dataframe as a .csv file into your local disk. \n",
    "\n",
    "Here are the column details:\n",
    "\n",
    "Column 1: Date\n",
    "\n",
    "Column 2: Post score\n",
    "\n",
    "Column 3: Post title\n",
    "\n",
    "Column 4: Top comment 1\n",
    "\n",
    "Column 5: Top comment 2\n",
    "\n",
    "Column 6: Top comment 3\n",
    "\n",
    "When you create the .csv file, it should have 101 rows (including column names) and 6 columns.\n",
    "\n",
    "**Grading criteria:**\n",
    "If your code produces a .csv file in the local disk in the same folder as the Jupyter Notebook file, you get full points. Otherwise, no point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee47057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 135 entries, 0 to 134\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       135 non-null    datetime64[ns]\n",
      " 1   Score      135 non-null    int64         \n",
      " 2   Title      135 non-null    object        \n",
      " 3   Top Com 1  135 non-null    object        \n",
      " 4   Top Com 2  135 non-null    object        \n",
      " 5   Top Com 3  135 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# your code for step 3 goes here\n",
    "import pandas as pd\n",
    "\n",
    "df_submission = pd.DataFrame(list(zip(list_submission_date, list_submission_score, list_submission_title,\n",
    "                                     list_submission_com1, list_submission_com2, list_submission_com3)), \n",
    "                             columns=['Date', 'Score', 'Title', 'Top Com 1', 'Top Com 2', 'Top Com 3' ])\n",
    "df_submission.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210e4927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Top Com 1</th>\n",
       "      <th>Top Com 2</th>\n",
       "      <th>Top Com 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-12 14:39:38</td>\n",
       "      <td>2646</td>\n",
       "      <td>how about that data integrity yo</td>\n",
       "      <td>If you find a good data engineer, you do every...</td>\n",
       "      <td>What are some examples of differences between ...</td>\n",
       "      <td>The true heroes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-14 03:00:03</td>\n",
       "      <td>2184</td>\n",
       "      <td>I created a four-page Data Science Cheatsheet ...</td>\n",
       "      <td>Nice work! Maybe consider adding another page ...</td>\n",
       "      <td>Doing the Lord’s work out here. Thank you so m...</td>\n",
       "      <td>Oh man, I have a test coming up in data analyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-08 19:22:16</td>\n",
       "      <td>1729</td>\n",
       "      <td>I just got offered a data science internship w...</td>\n",
       "      <td>Congrats on the offer. Amazon is a great first...</td>\n",
       "      <td>Mind sharing some of the cheat sheets?</td>\n",
       "      <td>congratulations! i recently switched to data s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-26 13:06:23</td>\n",
       "      <td>1630</td>\n",
       "      <td>Me showing off a suspiciously well-performing ...</td>\n",
       "      <td>At a corporate presentation a consultant showc...</td>\n",
       "      <td>Pls respond</td>\n",
       "      <td>Thanks for checking out the comic! This idea c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-25 13:19:39</td>\n",
       "      <td>1396</td>\n",
       "      <td>Alan Turing is the new face on the British £50...</td>\n",
       "      <td>The terrible things this genius went through :'(</td>\n",
       "      <td>Perhaps the man who both contributed more to c...</td>\n",
       "      <td>What are some other computer scientists like A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-08-18 06:34:05</td>\n",
       "      <td>1371</td>\n",
       "      <td>Very proud of my CS book collection.</td>\n",
       "      <td>How many of those have you read? Which would y...</td>\n",
       "      <td>Top tip, if you sleep on it the information wi...</td>\n",
       "      <td>I see the hungry caterpillar sneaking in there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-12 04:09:25</td>\n",
       "      <td>1217</td>\n",
       "      <td>I found a research paper that is almost entire...</td>\n",
       "      <td>Send an email to the editor of the journal. In...</td>\n",
       "      <td>Holy shit they aren't even trying to hide it. ...</td>\n",
       "      <td>Yeah, this is bizarre. It's not a published jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-19 16:01:05</td>\n",
       "      <td>1202</td>\n",
       "      <td>The Key Word in Data Science is Science, not Data</td>\n",
       "      <td>I can't tell you how many times I've backed ou...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>It is also important to understand what that \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-06-20 13:58:29</td>\n",
       "      <td>1131</td>\n",
       "      <td>Hi! I just expanded the Data Science Cheatshee...</td>\n",
       "      <td>Wow! Thank you!</td>\n",
       "      <td>Jumping in to say that your sheet just might h...</td>\n",
       "      <td>This is an excellent resource for reviewing ML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-06-07 14:33:48</td>\n",
       "      <td>1019</td>\n",
       "      <td>Data Science and Data Analytics is becoming ul...</td>\n",
       "      <td>Lots of companies also think that Data Scienti...</td>\n",
       "      <td>I’m actually considering switching to data eng...</td>\n",
       "      <td>Data science is different now (https://veekayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-06-01 14:03:43</td>\n",
       "      <td>954</td>\n",
       "      <td>I’m so sick of corporate morons</td>\n",
       "      <td>For awhile I felt like 80% of my job was convi...</td>\n",
       "      <td>The fact that you even know what you think you...</td>\n",
       "      <td>More stories please \\nI love these kinds rants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-02-15 21:04:33</td>\n",
       "      <td>936</td>\n",
       "      <td>Please STOP asking Data Scientists about Leetc...</td>\n",
       "      <td>I’m a software engineer who worked on a data s...</td>\n",
       "      <td>I’m not surprised most companies get little to...</td>\n",
       "      <td>To be fair, Leetcode style questions have litt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-02-10 01:33:48</td>\n",
       "      <td>916</td>\n",
       "      <td>Remember to stop every once in a while and thi...</td>\n",
       "      <td>Yep. I used to loop through rows of data frame...</td>\n",
       "      <td>In my last project, I was actually the “least ...</td>\n",
       "      <td>Wholesome. Cheers mate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-05-28 13:59:36</td>\n",
       "      <td>916</td>\n",
       "      <td>First two weeks of my first internship</td>\n",
       "      <td>Congratulations!! Keep us updated on your prof...</td>\n",
       "      <td>I feel you man.\\n\\nI came to Europe from a cou...</td>\n",
       "      <td>Congratulations on that. I am literally in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-03-12 14:27:28</td>\n",
       "      <td>851</td>\n",
       "      <td>Can't land a data internship? Try volunteering...</td>\n",
       "      <td>I have to say I would have never thought of it...</td>\n",
       "      <td>This is thinking out of the box. Great suggest...</td>\n",
       "      <td>I work for a company that does a lot of work w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-08-26 16:00:39</td>\n",
       "      <td>846</td>\n",
       "      <td>Help me understand what I’m doing wrong</td>\n",
       "      <td>Today is not Meme Monday, but I'll allow it.</td>\n",
       "      <td>Okay so first off, wait for a rainy day. Data,...</td>\n",
       "      <td>You're approaching the wrong people. Go find p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-08-11 09:02:08</td>\n",
       "      <td>833</td>\n",
       "      <td>An interesting job posting I found for a Work ...</td>\n",
       "      <td>Bruh, that's a lot to ask for free</td>\n",
       "      <td>Gain access to GIGABYTES of data? How exciting!</td>\n",
       "      <td>Am I free to post all their data on internet t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-02-23 18:14:46</td>\n",
       "      <td>765</td>\n",
       "      <td>My first technical interview experience(22+ in...</td>\n",
       "      <td>I would bomb this, my spot recollection of spe...</td>\n",
       "      <td>I think for #2 he just wanted you to explicitl...</td>\n",
       "      <td>Great post.  I can tell you, as someone who ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-02-06 11:00:14</td>\n",
       "      <td>756</td>\n",
       "      <td>Is anybody else here trying to actively push b...</td>\n",
       "      <td>If you don't already have a team of data engin...</td>\n",
       "      <td>I had an interview for a DS position the other...</td>\n",
       "      <td>This is true in all technology (obviously a ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-04-02 15:31:58</td>\n",
       "      <td>747</td>\n",
       "      <td>Against the negativity here, I just received m...</td>\n",
       "      <td>The negativity is mostly around entry-level jo...</td>\n",
       "      <td>Yeah, FAppleNG DS roles pay 200k base at staff...</td>\n",
       "      <td>I got a job offer back in late July for $230K,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-07-10 23:06:12</td>\n",
       "      <td>722</td>\n",
       "      <td>Anyone else cringe when faced with working wit...</td>\n",
       "      <td>There should be five years of real world work ...</td>\n",
       "      <td>There are good people with MBAs and there are ...</td>\n",
       "      <td>Well i dont like dipshits of any major but if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-05-03 12:53:21</td>\n",
       "      <td>704</td>\n",
       "      <td>I'm a Senior Data Scientist at Disney and I'm ...</td>\n",
       "      <td>Is Susan a nickname?</td>\n",
       "      <td>If you are interested in seeing what these ses...</td>\n",
       "      <td>Please, please, pretty please record this. I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-07-26 23:17:29</td>\n",
       "      <td>696</td>\n",
       "      <td>I translated it from Prussian for y'all</td>\n",
       "      <td>The original quote was from von Moltke the eld...</td>\n",
       "      <td>If your offensive goes as planned, then unmist...</td>\n",
       "      <td>Prussians invented multi threading (fighting b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-08-09 20:38:16</td>\n",
       "      <td>690</td>\n",
       "      <td>What being a data scientist on LinkedIn looks ...</td>\n",
       "      <td>I recognize an fstring when I see one in the w...</td>\n",
       "      <td>Congratulations First Name, sounds like a real...</td>\n",
       "      <td>I'm not convinced my profile got reviewed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-01-14 06:56:02</td>\n",
       "      <td>676</td>\n",
       "      <td>We Need More Data Engineers, Not Data Scientists</td>\n",
       "      <td>There is a rather clear shift in the market in...</td>\n",
       "      <td>We also need data scientists that focus on sta...</td>\n",
       "      <td>Software engineer here that has been doing dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-07-20 01:29:22</td>\n",
       "      <td>679</td>\n",
       "      <td>FYI: If You're New to the Industry, the Data S...</td>\n",
       "      <td>In my opinion, it is much safer to develop exp...</td>\n",
       "      <td>And many of them aren’t worth hiring.</td>\n",
       "      <td>The industry is the wild west. Everyone sees t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-07-10 05:08:51</td>\n",
       "      <td>669</td>\n",
       "      <td>Low key the new icon kinda sucks</td>\n",
       "      <td>Hey, thanks for the discussion thread! I am th...</td>\n",
       "      <td>The floppy disk? Is that the logo? Doesn't mak...</td>\n",
       "      <td>Sounds like it's time to do some A/B testing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-03-17 14:22:29</td>\n",
       "      <td>660</td>\n",
       "      <td>Imposter syndrome and prioritizing what to learn</td>\n",
       "      <td>Great post!\\n\\nI would add that, particularly ...</td>\n",
       "      <td>This resonates strongly with me. Thank you for...</td>\n",
       "      <td>Also- ask your boss. My company is big on angu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-05-27 15:51:14</td>\n",
       "      <td>656</td>\n",
       "      <td>A lot of people entering this field are like o...</td>\n",
       "      <td>Oh come on. Have you read job descriptions?\\n\\...</td>\n",
       "      <td>Overfitting gets stakeholders’ initial buy in....</td>\n",
       "      <td>Showerthoughts by data people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-01-19 14:34:08</td>\n",
       "      <td>655</td>\n",
       "      <td>I'm a Senior Data Scientist at Disney and I'm ...</td>\n",
       "      <td>There seems to be issues with the link in the ...</td>\n",
       "      <td>Is Disney still outsourcing jobs, or perhaps t...</td>\n",
       "      <td>Thank you for taking time out to do this.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date  Score  \\\n",
       "0  2021-07-12 14:39:38   2646   \n",
       "1  2021-02-14 03:00:03   2184   \n",
       "2  2021-04-08 19:22:16   1729   \n",
       "3  2021-07-26 13:06:23   1630   \n",
       "4  2021-03-25 13:19:39   1396   \n",
       "5  2021-08-18 06:34:05   1371   \n",
       "6  2021-04-12 04:09:25   1217   \n",
       "7  2021-08-19 16:01:05   1202   \n",
       "8  2021-06-20 13:58:29   1131   \n",
       "9  2021-06-07 14:33:48   1019   \n",
       "10 2021-06-01 14:03:43    954   \n",
       "11 2021-02-15 21:04:33    936   \n",
       "12 2021-02-10 01:33:48    916   \n",
       "13 2021-05-28 13:59:36    916   \n",
       "14 2021-03-12 14:27:28    851   \n",
       "15 2021-08-26 16:00:39    846   \n",
       "16 2021-08-11 09:02:08    833   \n",
       "17 2021-02-23 18:14:46    765   \n",
       "18 2021-02-06 11:00:14    756   \n",
       "19 2021-04-02 15:31:58    747   \n",
       "20 2021-07-10 23:06:12    722   \n",
       "21 2021-05-03 12:53:21    704   \n",
       "22 2021-07-26 23:17:29    696   \n",
       "23 2021-08-09 20:38:16    690   \n",
       "24 2021-01-14 06:56:02    676   \n",
       "25 2021-07-20 01:29:22    679   \n",
       "26 2021-07-10 05:08:51    669   \n",
       "27 2021-03-17 14:22:29    660   \n",
       "28 2021-05-27 15:51:14    656   \n",
       "29 2021-01-19 14:34:08    655   \n",
       "\n",
       "                                                Title  \\\n",
       "0                    how about that data integrity yo   \n",
       "1   I created a four-page Data Science Cheatsheet ...   \n",
       "2   I just got offered a data science internship w...   \n",
       "3   Me showing off a suspiciously well-performing ...   \n",
       "4   Alan Turing is the new face on the British £50...   \n",
       "5                Very proud of my CS book collection.   \n",
       "6   I found a research paper that is almost entire...   \n",
       "7   The Key Word in Data Science is Science, not Data   \n",
       "8   Hi! I just expanded the Data Science Cheatshee...   \n",
       "9   Data Science and Data Analytics is becoming ul...   \n",
       "10                    I’m so sick of corporate morons   \n",
       "11  Please STOP asking Data Scientists about Leetc...   \n",
       "12  Remember to stop every once in a while and thi...   \n",
       "13             First two weeks of my first internship   \n",
       "14  Can't land a data internship? Try volunteering...   \n",
       "15            Help me understand what I’m doing wrong   \n",
       "16  An interesting job posting I found for a Work ...   \n",
       "17  My first technical interview experience(22+ in...   \n",
       "18  Is anybody else here trying to actively push b...   \n",
       "19  Against the negativity here, I just received m...   \n",
       "20  Anyone else cringe when faced with working wit...   \n",
       "21  I'm a Senior Data Scientist at Disney and I'm ...   \n",
       "22            I translated it from Prussian for y'all   \n",
       "23  What being a data scientist on LinkedIn looks ...   \n",
       "24   We Need More Data Engineers, Not Data Scientists   \n",
       "25  FYI: If You're New to the Industry, the Data S...   \n",
       "26                   Low key the new icon kinda sucks   \n",
       "27   Imposter syndrome and prioritizing what to learn   \n",
       "28  A lot of people entering this field are like o...   \n",
       "29  I'm a Senior Data Scientist at Disney and I'm ...   \n",
       "\n",
       "                                            Top Com 1  \\\n",
       "0   If you find a good data engineer, you do every...   \n",
       "1   Nice work! Maybe consider adding another page ...   \n",
       "2   Congrats on the offer. Amazon is a great first...   \n",
       "3   At a corporate presentation a consultant showc...   \n",
       "4    The terrible things this genius went through :'(   \n",
       "5   How many of those have you read? Which would y...   \n",
       "6   Send an email to the editor of the journal. In...   \n",
       "7   I can't tell you how many times I've backed ou...   \n",
       "8                                     Wow! Thank you!   \n",
       "9   Lots of companies also think that Data Scienti...   \n",
       "10  For awhile I felt like 80% of my job was convi...   \n",
       "11  I’m a software engineer who worked on a data s...   \n",
       "12  Yep. I used to loop through rows of data frame...   \n",
       "13  Congratulations!! Keep us updated on your prof...   \n",
       "14  I have to say I would have never thought of it...   \n",
       "15       Today is not Meme Monday, but I'll allow it.   \n",
       "16                 Bruh, that's a lot to ask for free   \n",
       "17  I would bomb this, my spot recollection of spe...   \n",
       "18  If you don't already have a team of data engin...   \n",
       "19  The negativity is mostly around entry-level jo...   \n",
       "20  There should be five years of real world work ...   \n",
       "21                               Is Susan a nickname?   \n",
       "22  The original quote was from von Moltke the eld...   \n",
       "23  I recognize an fstring when I see one in the w...   \n",
       "24  There is a rather clear shift in the market in...   \n",
       "25  In my opinion, it is much safer to develop exp...   \n",
       "26  Hey, thanks for the discussion thread! I am th...   \n",
       "27  Great post!\\n\\nI would add that, particularly ...   \n",
       "28  Oh come on. Have you read job descriptions?\\n\\...   \n",
       "29  There seems to be issues with the link in the ...   \n",
       "\n",
       "                                            Top Com 2  \\\n",
       "0   What are some examples of differences between ...   \n",
       "1   Doing the Lord’s work out here. Thank you so m...   \n",
       "2              Mind sharing some of the cheat sheets?   \n",
       "3                                         Pls respond   \n",
       "4   Perhaps the man who both contributed more to c...   \n",
       "5   Top tip, if you sleep on it the information wi...   \n",
       "6   Holy shit they aren't even trying to hide it. ...   \n",
       "7                                           [deleted]   \n",
       "8   Jumping in to say that your sheet just might h...   \n",
       "9   I’m actually considering switching to data eng...   \n",
       "10  The fact that you even know what you think you...   \n",
       "11  I’m not surprised most companies get little to...   \n",
       "12  In my last project, I was actually the “least ...   \n",
       "13  I feel you man.\\n\\nI came to Europe from a cou...   \n",
       "14  This is thinking out of the box. Great suggest...   \n",
       "15  Okay so first off, wait for a rainy day. Data,...   \n",
       "16    Gain access to GIGABYTES of data? How exciting!   \n",
       "17  I think for #2 he just wanted you to explicitl...   \n",
       "18  I had an interview for a DS position the other...   \n",
       "19  Yeah, FAppleNG DS roles pay 200k base at staff...   \n",
       "20  There are good people with MBAs and there are ...   \n",
       "21  If you are interested in seeing what these ses...   \n",
       "22  If your offensive goes as planned, then unmist...   \n",
       "23  Congratulations First Name, sounds like a real...   \n",
       "24  We also need data scientists that focus on sta...   \n",
       "25              And many of them aren’t worth hiring.   \n",
       "26  The floppy disk? Is that the logo? Doesn't mak...   \n",
       "27  This resonates strongly with me. Thank you for...   \n",
       "28  Overfitting gets stakeholders’ initial buy in....   \n",
       "29  Is Disney still outsourcing jobs, or perhaps t...   \n",
       "\n",
       "                                            Top Com 3  \n",
       "0                                     The true heroes  \n",
       "1   Oh man, I have a test coming up in data analyt...  \n",
       "2   congratulations! i recently switched to data s...  \n",
       "3   Thanks for checking out the comic! This idea c...  \n",
       "4   What are some other computer scientists like A...  \n",
       "5   I see the hungry caterpillar sneaking in there...  \n",
       "6   Yeah, this is bizarre. It's not a published jo...  \n",
       "7   It is also important to understand what that \"...  \n",
       "8   This is an excellent resource for reviewing ML...  \n",
       "9   Data science is different now (https://veekayb...  \n",
       "10     More stories please \\nI love these kinds rants  \n",
       "11  To be fair, Leetcode style questions have litt...  \n",
       "12                            Wholesome. Cheers mate.  \n",
       "13  Congratulations on that. I am literally in the...  \n",
       "14  I work for a company that does a lot of work w...  \n",
       "15  You're approaching the wrong people. Go find p...  \n",
       "16  Am I free to post all their data on internet t...  \n",
       "17  Great post.  I can tell you, as someone who ha...  \n",
       "18  This is true in all technology (obviously a ge...  \n",
       "19  I got a job offer back in late July for $230K,...  \n",
       "20  Well i dont like dipshits of any major but if ...  \n",
       "21  Please, please, pretty please record this. I c...  \n",
       "22  Prussians invented multi threading (fighting b...  \n",
       "23         I'm not convinced my profile got reviewed.  \n",
       "24  Software engineer here that has been doing dat...  \n",
       "25  The industry is the wild west. Everyone sees t...  \n",
       "26      Sounds like it's time to do some A/B testing.  \n",
       "27  Also- ask your boss. My company is big on angu...  \n",
       "28                      Showerthoughts by data people  \n",
       "29          Thank you for taking time out to do this.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb7de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_submission.to_csv('DS_reddit_top_post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8aebc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date:  2021-01-25 22:27:16\n",
      "Score:  380\n",
      "Title:  Did anyone regret choosing DS as a career or has got disillusioned with it?\n",
      "Top Com1:  I am in no way as experienced as you. However, I have heard from a VP of Data who told me that he formed his department to have 3 paths. One path is to become a manager of junior data analysts if they enjoy managing people. One path is to become an ML expert if they enjoy the technical aspect. One path is to become a business analyst if they enjoy the product/business aspect. \n",
      "\n",
      "From your post, it sounds like you went down the ML expert route but you're unhappy with it. Data Science is so broad, that you won't have to exit DS completely to find enjoyment in your work. I think if you wanted to, you could try to transfer your skills into the other paths. Maybe in those paths will you find out whether you're meat or fish!\n",
      "Top Com2:  I don't regret it, since it's a safe job and pays better than most careers.\n",
      "\n",
      "However, I can empathize with the lonely/unfulfilling aspect. I mostly sit isolated, coding & developing dashboards for various tasks handed to me. Other folks in my org generally speak to my work and communicate internally, and to 3rd parties. \n",
      "\n",
      "I do tire from it, too. It's challenging, yet boring at the same time. I grew up in the outdoors, and used to assist hiking expeditions in the Rockies....so part of me thinks that most jobs aren't so exciting after that. All about perspective, I suppose.\n",
      "Top Com3:  I can identify with this. I got into the field because I love model building and really wanted to make a difference with my career (Thus I picked a particular type of consulting that would let me work on high impact projects! And for a few years - I did!). \n",
      "\n",
      "Now a decade in, I've found I routinely get shunted into sales, management, and high-value type projects instead of impactful ones. And no matter how I look at it, helping to optimize how much money some arbitrary company makes has never been as appealing to me. Now that I'm managing a team, I also find most of my day is just meetings and powerpoints. \n",
      "\n",
      "My solution has been to pursue a Ph.D. which has allowed me to work on super fun research that actually has an impact. I'm planning to stay in the academic realm after, but if things go belly up my game plan is to find a remote-well-paid data science job. Because honestly if I had to \"tolerate\" running models from my couch while occasionally playing video games in between coding, I could do that...\n"
     ]
    }
   ],
   "source": [
    "#looking for insightful posts\n",
    "print('Date: ', df_submission.at[83, 'Date'])\n",
    "print('Score: ', df_submission.at[83, 'Score'])\n",
    "print('Title: ', df_submission.at[83, 'Title'])\n",
    "print('Top Com1: ', df_submission.at[83, 'Top Com 1'])\n",
    "print('Top Com2: ', df_submission.at[83, 'Top Com 2'])\n",
    "print('Top Com3: ', df_submission.at[83, 'Top Com 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b12bd",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "### 10 points\n",
    "#### Presentation slides:\n",
    "   \n",
    "Create presentation slides for this case study. The presentation slides should provide an overview of the problem you tried to solve, methods you have used (don't put actual code in the slides), and if you have discovered new insights from the data you have collected. You may put actual post titles or comments in the slide that you found insightful. The number of slides should be around 6-7 (no hard limit). Three of you will be randomly chosen and be asked to present your work in the class. You should be prepared to present your work for 5 mins.\n",
    "\n",
    "**Notes on grading**: 5 points will be deducted if you are not prepared to present on the day of submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e062707",
   "metadata": {},
   "source": [
    "### What to submit:\n",
    "\n",
    "Put the Jupyter Notebook file and the .csv file in a folder. Then convert your presentation slides in to a PDF file and put it in the same folder. Zip the folder. After zipping, it should have the extension .zip. The name of the .zip file should be firstname_lastname_casestudy_1.zip . Upload the .zip file on Canvas.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
